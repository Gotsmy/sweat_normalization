{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(1)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit, minimize\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import tqdm\n",
    "import random\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import robust_loss_pytorch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caf(tt,k1,k2,k3,k4,xxx0):\n",
    "    y = ((-np.exp(-(k3+k2)*tt)+np.exp(-k1*tt))*k1)/(k3+k2-k1)\n",
    "    return y\n",
    "\n",
    "def xxx(tt,k1,k2,k3,k4,xxx0):\n",
    "    y = (np.exp(-(k1+k4)*tt)*(-np.exp(k4*tt)*k2*k1*(k3+k2-k4)+np.exp((-k3-k2+k1+k4)*tt)*k2*k1*(k1-k4)-np.exp(k1*tt)*(-k3-k2+k1)*((k3-k4)*(k1-k4)*xxx0+k2*(k1+k1*xxx0-k4*xxx0))))/((k3+k2-k1)*(k3+k2-k4)*(k1-k4))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imputed_data(donors):\n",
    "    '''\n",
    "    Loads (normalized by machine standard) raw data from a list of donors.\n",
    "    Returns DataFrames in a dictionary.\n",
    "    '''\n",
    "    \n",
    "    df = pd.read_csv('/home/users/mgotsmy/sweat/210000_notebooks/210706_A_train_test_set/train_data/4_with_metadata.csv',index_col=0)\n",
    "    data = {}\n",
    "    for donor in donors:\n",
    "        tmp = df[df['donor']==donor]\n",
    "        data[donor] = tmp.loc[:,['time','Caffeine','Paraxanthine','Theobromine','Theophylline']]\n",
    "    \n",
    "    return data\n",
    "\n",
    "def optimization_problem_robust_loss_production(_input):\n",
    "    '''\n",
    "    Uses _input list of donor_name, sampled p0, and bounds (lb, ub). Calls curve_fit() and writes out the fitted parameters.\n",
    "    '''\n",
    "    \n",
    "    # the loss parameters alpha and scale are initially estimated as 1\n",
    "    global alpha, scale, loss\n",
    "    alpha, scale = 1, 1\n",
    "    # reading the input\n",
    "    donor, p0, lb, ub, metabolite = _input\n",
    "    # doing data preprocessing\n",
    "    x_values, y_values, s_values = preprocessing_imputed_data(data,donor,metabolite)\n",
    "\n",
    "    # in rare cases initial parameters can lead to NaNs which can create errors. This is excepted here.\n",
    "    try:\n",
    "        para, var = curve_fit(f         = fit_with_sv,\n",
    "                              xdata     = x_values,\n",
    "                              ydata     = y_values,\n",
    "                              sigma     = s_values,\n",
    "                              p0        = p0,\n",
    "                              bounds    = (lb.astype('float64') ,ub.astype('float64')),\n",
    "                              method    = 'trf',\n",
    "                              max_nfev  = 1000,\n",
    "                              loss      = max_cauchy_loss,\n",
    "                              tr_solver = 'exact')\n",
    "        observed = y_values\n",
    "        expected = fit_with_sv(x_values,*para)\n",
    "        loss = np.sum(max_cauchy_loss((np.abs(observed-expected)/s_values)**2)[0])\n",
    "        return [donor,metabolite, list(para), float(loss)]\n",
    "    except (ValueError,RuntimeError) as e:\n",
    "        return '#Error'\n",
    "    \n",
    "def optimization_problem_robust_loss_debug(_input):\n",
    "    '''\n",
    "    Uses _input list of donor_name, sampled p0, and bounds (lb, ub). Calls curve_fit() and writes out the fitted parameters.\n",
    "    '''\n",
    "    \n",
    "    # the loss parameters alpha and scale are initially estimated as 1\n",
    "    global alpha, scale, loss\n",
    "    alpha, scale = 1, 1\n",
    "    # reading the input\n",
    "    donor, p0, lb, ub, metabolite = _input\n",
    "    # doing data preprocessing\n",
    "    x_values, y_values, s_values = preprocessing_imputed_data(data,donor,metabolite)\n",
    "\n",
    "    # in rare cases initial parameters can lead to NaNs which can create errors. This is excepted here.\n",
    "#     try:\n",
    "    para, var = curve_fit(f         = fit_with_sv,\n",
    "                          xdata     = x_values,\n",
    "                          ydata     = y_values,\n",
    "                          sigma     = s_values,\n",
    "                          p0        = p0,\n",
    "                          bounds    = (lb.astype('float64') ,ub.astype('float64')),\n",
    "                          method    = 'trf',\n",
    "                          max_nfev  = 1000,\n",
    "                          loss      = max_cauchy_loss,\n",
    "                          tr_solver = 'exact')\n",
    "    observed = y_values\n",
    "    expected = fit_with_sv(x_values,*para)\n",
    "    loss = np.sum(max_cauchy_loss((np.abs(observed-expected)/s_values)**2)[0])\n",
    "    return [donor,metabolite, list(para), float(loss)]\n",
    "#     except (ValueError,RuntimeError) as e:\n",
    "#         return '#Error'\n",
    "\n",
    "def preprocessing_imputed_data(data,donor,metabolite):\n",
    "    '''\n",
    "    Takes raw data dictionary and donor name.\n",
    "    Parses x_values and y_values.\n",
    "    Transforms y_values by calibration curve.\n",
    "    Assigns 10**8 sigma values (= weights) to all 0s because they are below LOQ and thus regarded as NaNs.\n",
    "    Returns x_values, sigma, y_values as float64 np.arrays.\n",
    "    '''\n",
    "    \n",
    "    x_values = data[donor]['time'].copy()\n",
    "    y_values = np.array(data[donor].loc[:,['Caffeine','Paraxanthine','Theobromine','Theophylline']].copy())\n",
    "    y_values = y_values.flatten()\n",
    "    \n",
    "    # apply calibration curve\n",
    "    calibration_k = np.array([np.ones(len(x_values))*1.520,np.ones(len(x_values))*1.656,np.ones(len(x_values))*2.050,np.ones(len(x_values))*1.592]).flatten('F')\n",
    "    y_values      = y_values*calibration_k\n",
    "\n",
    "    # pharmacological factors\n",
    "    vdist_avail = 0.579                                        # volume of distribution/ bioavailability of caffeine\n",
    "    vsample     = 123                                          # sample volume in µL\n",
    "    dose        = 200*10**3                                    # caffeine dose in µg\n",
    "    body_mass   = {'Donor_16': 72, 'Donor_6': 83, 'Donor_19': 83, 'Donor_17': 52, 'Donor_18': 66, 'Donor_7': 66, 'Donor_3': 57, 'Donor_4': 84, 'Donor_5': 82.5, 'Donor_2': 77, 'Donor_8': 80, 'Donor_9': 83, 'Donor_10': 55, 'Donor_11': 54, 'Donor_1': 70, 'Donor_20': 105, 'Donor_21': 70, 'Donor_22': 64, 'Donor_23': 57, 'Donor_24': 60, 'Donor_25': 80, 'Donor_26': 86, 'Donor_27': 68, 'Donor_28': 82, 'Donor_29': 60, 'Donor_30': 55, 'Donor_31': 80, 'Donor_32': 92, 'Donor_33': 92, 'Donor_34': 71, 'Donor_35': 80, 'Donor_36': 77, 'Donor_37': 66, 'Donor_38': 63, 'Donor_39': 75, 'Donor_40': 75, 'Donor_41': 57, 'Donor_42': 75, 'Donor_43': 99, 'Donor_44': 75, 'Donor_45': 61, 'Donor_46': 52, 'Donor_47': 80}\n",
    "    factor      = vsample*body_mass[donor]*vdist_avail/dose\n",
    "    y_values    = y_values*factor\n",
    "    \n",
    "    \n",
    "\n",
    "    if metabolite == 'Paraxanthine':\n",
    "        i = 1\n",
    "    elif metabolite == 'Theobromine':\n",
    "        i = 2\n",
    "    elif metabolite == 'Theophylline':\n",
    "        i = 3\n",
    "    else:\n",
    "        print('unkown metabolite:',metabolite)\n",
    "    \n",
    "    # applying lambda \n",
    "    sigma   = []\n",
    "    if lambda_method == 'n_PQN':\n",
    "        n_lambda = len(pqn_dic[donor][metabolite])\n",
    "    elif lambda_method == 'n_EM':\n",
    "        n_lambda = 2\n",
    "    else:\n",
    "        print('Unknown lambda_method:',lambda_method)\n",
    "    lambda_ = 1/(1+n_lambda)\n",
    "    concentration_sigma = np.ones(len(y_values))/lambda_\n",
    "    pqn_sigma           = np.ones(len(pqn_dic[donor][metabolite]))/(1-lambda_)\n",
    "    y_values = np.concatenate([y_values.reshape(-1,4)[:,[0,i]].reshape(-1),pqn_dic[donor][metabolite]])\n",
    "    s_values = np.concatenate([concentration_sigma.reshape(-1,4)[:,[0,i]].reshape(-1),pqn_sigma]) \n",
    "    \n",
    "    \n",
    "    return x_values.astype('float64'), y_values.astype('float64'), s_values.astype('float64')\n",
    "\n",
    "def save_data(output_list,filepath):\n",
    "    '''\n",
    "    Takes list of fitting results and file path.\n",
    "    Saves fitting results row-wise in the given path.\n",
    "    '''\n",
    "    \n",
    "    with open(filepath,'w') as file:\n",
    "        for ol in output_list:\n",
    "            file.write(str(ol).replace('[','').replace(']','').replace(\"'\",'').replace(',','')+'\\n')\n",
    "    return\n",
    "\n",
    "def fit_with_sv(tt,x,k1,k2,k3,k4,xxx0,*sfs):\n",
    "    '''\n",
    "    This function takes timepoints (tt) and all fitting parameters and calculates original values (c*V_sweat) with it. \n",
    "    '''\n",
    "    \n",
    "    # global_args and global_timepoints are needed for the calculation of relative fitting error\n",
    "    global global_args, global_timepoints, global_sfs, global_x\n",
    "    args = [k1,k2,k3,k4,xxx0]\n",
    "    global_args = args\n",
    "    global_timepoints = tt\n",
    "    global_sfs = sfs\n",
    "    global_x = x\n",
    "    \n",
    "    # calculated c*V_sweat\n",
    "    caf_true = np.array(caf(tt,*args))*np.array(sfs)\n",
    "    xxx_true = np.array(xxx(tt,*args))*np.array(sfs)\n",
    "    y = np.array([caf_true,xxx_true])\n",
    "    y = np.concatenate([y.flatten('F'),np.array(sfs)*x])\n",
    "    return y    \n",
    "\n",
    "def fit_without_sv(tt,x,k1,k2,k3,k4,xxx0,*sfs):\n",
    "    '''\n",
    "    This function takes timepoints and all fitting parameters and calculates concentrations (c) with it. \n",
    "    Only for single measurements per timepoint!\n",
    "    '''\n",
    "    args = [k1,k2,k3,k4,xxx0]\n",
    "    caf_true = np.array(caf(tt,*args))\n",
    "    xxx_true = np.array(xxx(tt,*args))\n",
    "    y = np.array([caf_true,xxx_true])\n",
    "    y = np.concatenate([y.flatten('F'),np.array(sfs)*x])\n",
    "    return y\n",
    "\n",
    "def max_cauchy_loss(z):\n",
    "    '''Takes array, calculates maximum of relative and absolute error and\n",
    "    calculates Cauchy loss [2].\n",
    "    '''\n",
    "\n",
    "    global alpha, scale, loss\n",
    "    abs_error = z\n",
    "    # calculate relative error, except absolute error is 0\n",
    "    y = fit_without_sv(global_timepoints,global_x,*global_args,*global_sfs)\n",
    "    rel_error = np.divide(z, y, out=z.copy(), where=y!=0)\n",
    "    # maximum error\n",
    "    z = np.maximum(abs_error,rel_error)\n",
    "\n",
    "    rho = np.empty((3,len(z)))\n",
    "    rho[0] = np.log1p(z)\n",
    "    t = 1 + z\n",
    "    rho[1] = 1 / t\n",
    "    rho[2] = -1 / t**2\n",
    "    return rho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculate pqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pqn(data):\n",
    "    # implemented according to http://dx.doi.org/10.1016/j.chroma.2014.08.050\n",
    "    ref = np.median(data,axis=1) \n",
    "    div = np.divide(data,ref[:,None])\n",
    "    pqn = np.median(div,axis=0)\n",
    "    return pqn\n",
    "\n",
    "# set rng seed\n",
    "np.random.seed(13)\n",
    "\n",
    "# load untargeted data\n",
    "df = pd.read_csv('/home/users/mgotsmy/sweat/210000_notebooks/210706_A_train_test_set/train_data/4_with_metadata.csv',index_col=0)\n",
    "donors = ['Donor_1', 'Donor_10', 'Donor_16', 'Donor_17', 'Donor_18', 'Donor_19', 'Donor_2', 'Donor_20', 'Donor_21', 'Donor_22', 'Donor_23', 'Donor_24', 'Donor_25', 'Donor_26', 'Donor_27', 'Donor_28', 'Donor_29', 'Donor_3', 'Donor_30', 'Donor_31', 'Donor_32', 'Donor_33', 'Donor_34', 'Donor_35', 'Donor_36', 'Donor_37', 'Donor_38', 'Donor_39', 'Donor_4', 'Donor_40', 'Donor_41', 'Donor_42', 'Donor_43', 'Donor_44', 'Donor_45', 'Donor_46', 'Donor_47', 'Donor_5', 'Donor_7', 'Donor_8', 'Donor_9']\n",
    "\n",
    "# create PQN normalizing factors for 3 caffeine degradation metabolites\n",
    "m_list = np.arange(58)\n",
    "pqn_dic = {}\n",
    "for donor in donors:\n",
    "    tmp = df[df['donor']==donor]\n",
    "    tmp = tmp.iloc[:,9:].values.T\n",
    "    np.random.shuffle(m_list)\n",
    "    split_lists = np.array_split(m_list,3)\n",
    "    pqn_dic[donor] = {}\n",
    "    for nr, metabolite in enumerate(['Paraxanthine','Theobromine','Theophylline']):\n",
    "        pqn = calculate_pqn(tmp[split_lists[nr],:])\n",
    "        pqn_dic[donor][metabolite] = pqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12300/12300 [07:56<00:00, 25.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Percentage 0.04\n"
     ]
    }
   ],
   "source": [
    "## sampling MC replicates\n",
    "\n",
    "# load raw data for all donors\n",
    "donors = ['Donor_1', 'Donor_10', 'Donor_16', 'Donor_17', 'Donor_18', 'Donor_19', 'Donor_2', 'Donor_20', 'Donor_21', 'Donor_22', 'Donor_23', 'Donor_24', 'Donor_25', 'Donor_26', 'Donor_27', 'Donor_28', 'Donor_29', 'Donor_3', 'Donor_30', 'Donor_31', 'Donor_32', 'Donor_33', 'Donor_34', 'Donor_35', 'Donor_36', 'Donor_37', 'Donor_38', 'Donor_39', 'Donor_4', 'Donor_40', 'Donor_41', 'Donor_42', 'Donor_43', 'Donor_44', 'Donor_45', 'Donor_46', 'Donor_47', 'Donor_5', 'Donor_7', 'Donor_8', 'Donor_9']\n",
    "data   = load_imputed_data(donors)\n",
    "# create output directory\n",
    "lambda_method = 'n_PQN' # n_EM, n_PQN\n",
    "path = f'MIX_sub_2_{lambda_method}'\n",
    "Path(path).mkdir(exist_ok=True)\n",
    "\n",
    "# generate MC replicates\n",
    "input_list  = []\n",
    "output_list = []\n",
    "\n",
    "for metabolite in ['Paraxanthine','Theobromine','Theophylline']:\n",
    "    for donor in donors:\n",
    "        # set random seed to donor number\n",
    "        random.seed(float(donor.split('_')[1]))\n",
    "        # number of sweat volumes that are needed for fitting\n",
    "        n_sf = len(data[donor].index)\n",
    "        # calculate the normalized c(0) for C(0) = 1 mg/L for all donors\n",
    "        dose        = 200*10**3    # caffeine dose\n",
    "        body_mass   = {'Donor_16': 72, 'Donor_6': 83, 'Donor_19': 83, 'Donor_17': 52, 'Donor_18': 66, 'Donor_7': 66, 'Donor_3': 57, 'Donor_4': 84, 'Donor_5': 82.5, 'Donor_2': 77, 'Donor_8': 80, 'Donor_9': 83, 'Donor_10': 55, 'Donor_11': 54, 'Donor_1': 70, 'Donor_20': 105, 'Donor_21': 70, 'Donor_22': 64, 'Donor_23': 57, 'Donor_24': 60, 'Donor_25': 80, 'Donor_26': 86, 'Donor_27': 68, 'Donor_28': 82, 'Donor_29': 60, 'Donor_30': 55, 'Donor_31': 80, 'Donor_32': 92, 'Donor_33': 92, 'Donor_34': 71, 'Donor_35': 80, 'Donor_36': 77, 'Donor_37': 66, 'Donor_38': 63, 'Donor_39': 75, 'Donor_40': 75, 'Donor_41': 57, 'Donor_42': 75, 'Donor_43': 99, 'Donor_44': 75, 'Donor_45': 61, 'Donor_46': 52, 'Donor_47': 80}\n",
    "        vdist_avail = 0.579          # specific volume of distribution for caffeine\n",
    "        factor      = (body_mass[donor]*vdist_avail*1000)/dose\n",
    "        # set system bounds\n",
    "        lb = np.concatenate(([0],np.zeros(5),np.ones(n_sf)*.05))                        # lower bounds\n",
    "        ub = np.concatenate(([10,10],np.ones(3)*.2,np.ones(1)*factor,np.ones(n_sf)*4))  # upper bounds\n",
    "\n",
    "        # sample MC replicates\n",
    "        n_try   = 0\n",
    "        n_tries = 100  # number of MC replicates used\n",
    "        while n_try < n_tries:\n",
    "            n_try += 1\n",
    "            p0     = [] # initial parameters for the fit\n",
    "            for l,u in zip(lb,ub):\n",
    "                p0.append(random.uniform(l,u)) # sample from an uniform distribution between the system bounds\n",
    "            input_list.append([donor,p0,lb,ub,metabolite])\n",
    "            \n",
    "# multiprocess fitting\n",
    "with Pool(processes = cpu_count()) as p: # by default all processors are used\n",
    "    err_count = 0\n",
    "    for _ in tqdm.tqdm(p.imap_unordered(optimization_problem_robust_loss_production,input_list),total=len(input_list)):\n",
    "        if _ == '#Error':\n",
    "            err_count += 1\n",
    "            pass\n",
    "        else:\n",
    "            output_list.append(_)\n",
    "print('Error Percentage {:2.2f}'.format(err_count/len(input_list)*100))\n",
    "\n",
    "# safe the results\n",
    "for metabolite in ['Paraxanthine','Theobromine','Theophylline']:\n",
    "    for donor in donors:\n",
    "        write_out = []\n",
    "        for i in output_list:\n",
    "            if i[0] == donor and i[1] == metabolite:\n",
    "                write_out.append(i[2:])\n",
    "        if len(write_out) > 0:\n",
    "            save_data(write_out,f'{path}/{donor}_{metabolite}.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Robust Loss (3.7)",
   "language": "python",
   "name": "robust_loss"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
